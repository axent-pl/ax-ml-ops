version: '3.4'

volumes:
  db-volume: {}
  s3-volume: {}
  af-volume: {}

services:

  db:
    build:
      context: services/db
    environment:
      - POSTGRES_PASSWORD=${DB_ADMIN_PASS}
      - DB_AIRFLOW_USER=${DB_AIRFLOW_USER}
      - DB_AIRFLOW_PASS=${DB_AIRFLOW_PASS}
      - DB_AIRFLOW_NAME=${DB_AIRFLOW_NAME}
      - DB_MLFLOW_USER=${DB_MLFLOW_USER}
      - DB_MLFLOW_PASS=${DB_MLFLOW_PASS}
      - DB_MLFLOW_NAME=${DB_MLFLOW_NAME}
      - DB_OPTUNA_USER=${DB_OPTUNA_USER}
      - DB_OPTUNA_PASS=${DB_OPTUNA_PASS}
      - DB_OPTUNA_NAME=${DB_OPTUNA_NAME}
    volumes:
      - db-volume:/var/lib/postgresql/data
    networks:
      - ax

  mq:
    build:
      context: services/rabbitmq
    environment:
      - RABBITMQ_DEFAULT_USER=${MQ_USER}
      - RABBITMQ_DEFAULT_PASS=${MQ_PASS}
    networks:
      - ax

  s3:
    build:
        context: services/s3
    command:
        - server
        - /home/shared
        - --console-address
        - ":9001"
    ports:
        - "9000:9000"
        - "9001:9001"
    volumes:
      - s3-volume:/data
    networks:
        - ax

  s3-config-job:
    restart: on-failure
    build:
        context: services/s3-config
    environment:
        - S3_HOST=s3
        - S3_PORT=9000
        - S3_BUCKET=${S3_BUCKET}
        - S3_AUTH_ROOT_KEY_ID=${S3_AUTH_ROOT_KEY_ID}
        - S3_AUTH_ROOT_SECRET_KEY=${S3_AUTH_ROOT_SECRET_KEY}
        - S3_AUTH_KEY_ID=${S3_AUTH_KEY_ID}
        - S3_AUTH_SECRET_KEY=${S3_AUTH_SECRET_KEY}
    networks:
        - ax

  mlflow:
    restart: on-failure
    build:
      context: services/mlflow
    environment:
      - DB_HOST=db
      - DB_MLFLOW_USER=${DB_MLFLOW_USER}
      - DB_MLFLOW_PASS=${DB_MLFLOW_PASS}
      - DB_MLFLOW_NAME=${DB_MLFLOW_NAME}
      - S3_HOST=s3
      - S3_PORT=9000
      - S3_BUCKET=${S3_BUCKET}
      - S3_AUTH_KEY_ID=${S3_AUTH_KEY_ID}
      - S3_AUTH_SECRET_KEY=${S3_AUTH_SECRET_KEY}
    ports:
      - 5000:5000
    networks:
      - ax

  airflow:
    restart: on-failure
    build:
      context: services/airflow
      args:
      - UID=${UID}
      - GID=${GID}
    environment:
      - AF_USER=${AF_USER}
      - AF_PASS=${AF_PASS}
      - MQ_HOST=mq
      - MQ_USER=${MQ_USER}
      - MQ_PASS=${MQ_PASS}
      - MLFLOW_HOST=mlflow
      - MLFLOW_PORT=5000
      - DB_HOST=db
      - DB_AIRFLOW_USER=${DB_AIRFLOW_USER}
      - DB_AIRFLOW_PASS=${DB_AIRFLOW_PASS}
      - DB_AIRFLOW_NAME=${DB_AIRFLOW_NAME}
      - DB_MLFLOW_USER=${DB_MLFLOW_USER}
      - DB_MLFLOW_PASS=${DB_MLFLOW_PASS}
      - DB_MLFLOW_NAME=${DB_MLFLOW_NAME}
      - DB_OPTUNA_USER=${DB_OPTUNA_USER}
      - DB_OPTUNA_PASS=${DB_OPTUNA_PASS}
      - DB_OPTUNA_NAME=${DB_OPTUNA_NAME}
      - S3_HOST=s3
      - S3_PORT=9000
      - S3_BUCKET=${S3_BUCKET}
      - S3_AUTH_KEY_ID=${S3_AUTH_KEY_ID}
      - S3_AUTH_SECRET_KEY=${S3_AUTH_SECRET_KEY}
    volumes:
      - ./pipelines:/home/airflow/pipelines
    ports:
      - 8000:8080
    networks:
      - ax

  airflow-scheduler:
    restart: on-failure
    build:
      context: services/airflow
      args:
      - UID=${UID}
      - GID=${GID}
    command: "scheduler"
    environment:
      - AF_USER=${AF_USER}
      - AF_PASS=${AF_PASS}
      - MQ_HOST=mq
      - MQ_USER=${MQ_USER}
      - MQ_PASS=${MQ_PASS}
      - MLFLOW_HOST=mlflow
      - MLFLOW_PORT=5000
      - DB_HOST=db
      - DB_AIRFLOW_USER=${DB_AIRFLOW_USER}
      - DB_AIRFLOW_PASS=${DB_AIRFLOW_PASS}
      - DB_AIRFLOW_NAME=${DB_AIRFLOW_NAME}
      - DB_MLFLOW_USER=${DB_MLFLOW_USER}
      - DB_MLFLOW_PASS=${DB_MLFLOW_PASS}
      - DB_MLFLOW_NAME=${DB_MLFLOW_NAME}
      - DB_OPTUNA_USER=${DB_OPTUNA_USER}
      - DB_OPTUNA_PASS=${DB_OPTUNA_PASS}
      - DB_OPTUNA_NAME=${DB_OPTUNA_NAME}
      - S3_HOST=s3
      - S3_PORT=9000
      - S3_BUCKET=${S3_BUCKET}
      - S3_AUTH_KEY_ID=${S3_AUTH_KEY_ID}
      - S3_AUTH_SECRET_KEY=${S3_AUTH_SECRET_KEY}
    volumes:
      - ./pipelines:/home/airflow/pipelines
    networks:
      - ax

  airflow-worker:
    restart: on-failure
    deploy:
      mode: replicated
      replicas: 3
    build:
      context: services/airflow
      args:
      - UID=${UID}
      - GID=${GID}
    command: "worker"
    environment:
      - AF_USER=${AF_USER}
      - AF_PASS=${AF_PASS}
      - MQ_HOST=mq
      - MQ_USER=${MQ_USER}
      - MQ_PASS=${MQ_PASS}
      - MLFLOW_HOST=mlflow
      - MLFLOW_PORT=5000
      - DB_HOST=db
      - DB_AIRFLOW_USER=${DB_AIRFLOW_USER}
      - DB_AIRFLOW_PASS=${DB_AIRFLOW_PASS}
      - DB_AIRFLOW_NAME=${DB_AIRFLOW_NAME}
      - DB_MLFLOW_USER=${DB_MLFLOW_USER}
      - DB_MLFLOW_PASS=${DB_MLFLOW_PASS}
      - DB_MLFLOW_NAME=${DB_MLFLOW_NAME}
      - DB_OPTUNA_USER=${DB_OPTUNA_USER}
      - DB_OPTUNA_PASS=${DB_OPTUNA_PASS}
      - DB_OPTUNA_NAME=${DB_OPTUNA_NAME}
      - S3_HOST=s3
      - S3_PORT=9000
      - S3_BUCKET=${S3_BUCKET}
      - S3_AUTH_KEY_ID=${S3_AUTH_KEY_ID}
      - S3_AUTH_SECRET_KEY=${S3_AUTH_SECRET_KEY}
    volumes:
      - ./pipelines:/home/airflow/pipelines
    networks:
      - ax

  optuna-dashboard:
    restart: on-failure
    build:
      context: services/optuna-dashboard
    environment:
      - DB_HOST=db
      - DB_OPTUNA_USER=${DB_OPTUNA_USER}
      - DB_OPTUNA_PASS=${DB_OPTUNA_PASS}
      - DB_OPTUNA_NAME=${DB_OPTUNA_NAME}
    ports:
      - 8080:8080
    networks:
      - ax

  jupyter:
    build:
      context: services/jupyter
    environment:
      - DB_HOST=db
      - DB_MLFLOW_USER=${DB_MLFLOW_USER}
      - DB_MLFLOW_PASS=${DB_MLFLOW_PASS}
      - DB_MLFLOW_NAME=${DB_MLFLOW_NAME}
      - JUPYTER_TOKEN=${NB_PASS}
      - NB_UID=${UID}
      - MLFLOW_HOST=mlflow
      - MLFLOW_PORT=5000
      - S3_HOST=s3
      - S3_PORT=9000
      - S3_BUCKET=${S3_BUCKET}
      - S3_AUTH_KEY_ID=${S3_AUTH_KEY_ID}
      - S3_AUTH_SECRET_KEY=${S3_AUTH_SECRET_KEY}
    ports:
      - 8800:8888
    volumes:
      - ./pipelines:/home/jovyan/pipelines:ro
      - ./notebooks:/home/jovyan/notebooks
    networks:
      - ax

networks:
  ax:
    driver: bridge